{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQzQHuVomdy45n5KEHMFNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aknip/Langchain-etc./blob/main/Langchain-Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Agent\n",
        "\n",
        "1. Run cells \"Setup for all agents and apps\"\n",
        "2. Run one of the Agent sections / cells\n",
        "3. Run Gradio app or CLI apps\n"
      ],
      "metadata": {
        "id": "_yJpe7D-EXzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup for all agents and apps"
      ],
      "metadata": {
        "id": "gOMfnUf3_oTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "IN_NOTEBOOK = any([\"jupyter-notebook\" in i for i in psutil.Process().parent().cmdline()])"
      ],
      "metadata": {
        "id": "K_sEe-A1S56l"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "afIkZ1hD4RYB",
        "outputId": "811c4a42-03cc-48a2-d6bc-c9cd9c5977df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The gradio extension is already loaded. To reload it, use:\n",
            "  %reload_ext gradio\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchainhub langchain_experimental google-search-results wikipedia openai gradio -q\n",
        "# !pip install google-search-results wikipedia -q\n",
        "%load_ext gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "if IN_NOTEBOOK:\n",
        "  CREDS = json.loads(getpass(\"Secrets (JSON string): \"))\n",
        "  os.environ['CREDS'] = json.dumps(CREDS)\n",
        "  CREDS = json.loads(os.getenv('CREDS'))"
      ],
      "metadata": {
        "id": "IWrCvpKaENeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool, AgentExecutor\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.utilities import SerpAPIWrapper, SQLDatabase, WikipediaAPIWrapper\n",
        "from langchain_experimental.sql import SQLDatabaseChain\n",
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.tools import BaseTool, StructuredTool\n",
        "from langchain.schema.agent import AgentFinish\n",
        "import os\n",
        "import langchain\n",
        "import openai\n",
        "import textwrap\n",
        "import gradio as gr\n",
        "import random\n",
        "import time"
      ],
      "metadata": {
        "id": "CEhw8Ye15BgC"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent 1: Wikpedia, Search, Music DB"
      ],
      "metadata": {
        "id": "eHa5SgMbz5fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download \"Chinook\" Music Sales DB\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "fname = 'chinook.zip'\n",
        "url = 'https://www.sqlitetutorial.net/wp-content/uploads/2018/03/' + fname\n",
        "r = requests.get(url)\n",
        "open(fname, 'wb').write(r.content)\n",
        "zipfile.ZipFile('chinook.zip').extractall()\n",
        "assert os.path.exists('chinook.db')"
      ],
      "metadata": {
        "id": "NE6D9WvpGL1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = CREDS['OpenAI']['v2']['credential']\n",
        "llm = ChatOpenAI(temperature=0, model='gpt-4-0613')\n",
        "search = SerpAPIWrapper(serpapi_api_key=CREDS['SERP-API']['key']['credential'])\n",
        "db = SQLDatabase.from_uri(\"sqlite:///chinook.db\")\n",
        "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about real-time events. You should ask targeted questions\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Wikipedia\",\n",
        "        func=wikipedia.run,\n",
        "        description=\"useful for when you need to answer questions about a big picture or background of something.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"MusicSales\",\n",
        "        func=db_chain.run,\n",
        "        description=\"useful for when you need to answer questions about mucis sales in a store. Should be strickly follow the tables info.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a useful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "llm_with_tools = llm.bind(\n",
        "    functions=[format_tool_to_openai_function(t) for t in tools]\n",
        ")\n",
        "\n",
        "agent = {\n",
        "    \"input\": lambda x: x[\"input\"],\n",
        "    \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps']),\n",
        "    \"chat_history\": lambda x: x[\"chat_history\"]\n",
        "} | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HcePPP4M5HPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent 2: Calculate insurance premium"
      ],
      "metadata": {
        "id": "EoYqbRg0_w19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = CREDS['OpenAI']['v2']['credential']\n",
        "llm = ChatOpenAI(temperature=0, model='gpt-4-0613')\n",
        "\n",
        "def calculate_insurance_premium(insured_sum: int, industry: str) -> float:\n",
        "    \"\"\"Calculate the premium for an insurance based on the maximum insured sum and the industry of the customer.\"\"\"\n",
        "    premium = insured_sum*0.25\n",
        "    return premium\n",
        "\n",
        "calculation_tool = StructuredTool.from_function(calculate_insurance_premium, return_direct=False) # True return result without additional text\n",
        "\n",
        "tools = [\n",
        "    calculation_tool\n",
        "]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a useful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "llm_with_tools = llm.bind(\n",
        "    functions=[format_tool_to_openai_function(t) for t in tools]\n",
        ")\n",
        "\n",
        "agent = {\n",
        "    \"input\": lambda x: x[\"input\"],\n",
        "    \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps']),\n",
        "    \"chat_history\": lambda x: x[\"chat_history\"]\n",
        "} | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()\n"
      ],
      "metadata": {
        "id": "vAzSbqz7_0_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Chat App for Agent"
      ],
      "metadata": {
        "id": "O_jGi0yQ-sEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory)\n",
        "\n",
        "#\n",
        "# Gradio app\n",
        "#\n",
        "\n",
        "# Theming\n",
        "theme = gr.themes.Default(\n",
        "    primary_hue=\"slate\" # , radius_size=gr.themes.Size(radius_sm=\"3px\", radius_xs=\"2px\", radius_xxs=\"1px\")\n",
        ")\n",
        "# Styling: Change max width\n",
        "css = \"\"\"\n",
        "  .gradio-container {max-width: 800px!important}\n",
        "  .vspacer1 {margin-top: 20px}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(theme=theme, css=css) as demo:\n",
        "\n",
        "    gr.Markdown(\"# Agent Chat\", elem_classes=\"vspacer1\")\n",
        "\n",
        "    with gr.Tab(\"Chat\"):\n",
        "      # https://www.gradio.app/docs/chatbot\n",
        "\n",
        "      chatbot = gr.Chatbot(bubble_full_width=False)\n",
        "      msg = gr.Textbox()\n",
        "      clear = gr.ClearButton([msg, chatbot])\n",
        "\n",
        "      # Init agent and memory\n",
        "      memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "      agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory)\n",
        "      messages = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n",
        "      #chatbot.append(None, \"How can I help you?\")\n",
        "\n",
        "      def ask(message, chat_history):\n",
        "          chat_history.append((message, None))\n",
        "          messages.append({\"role\": \"user\", \"content\": message})\n",
        "          return \"\", chat_history\n",
        "\n",
        "      def respond(chat_history):\n",
        "          prompt = chat_history[-1][0] # get prompt from history (last entry)\n",
        "          response = agent_executor.invoke({\"input\": prompt})\n",
        "          msg = {\"role\": \"assistant\", \"content\": response[\"output\"]}\n",
        "          messages.append(msg)\n",
        "          chat_history.append((None, response[\"output\"]))\n",
        "          print(\"\\n\\nMemory from response object:\")\n",
        "          print(textwrap.fill(str(response[\"chat_history\"]), 80))\n",
        "          return chat_history\n",
        "\n",
        "      msg.submit(ask, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        respond, chatbot, chatbot\n",
        "      )\n",
        "\n",
        "demo.launch(quiet=True, share=False, debug=True)"
      ],
      "metadata": {
        "id": "kioXkMb2-vUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLI für agent_executor (Single Prompt)"
      ],
      "metadata": {
        "id": "dij8ZZWdbC5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False,  memory=memory)\n",
        "\n",
        "langchain.debug = False\n",
        "\n",
        "prompt = \"Give me the premium for an insured sum of 10000000 for metal construcion industry\"\n",
        "# prompt = \"Wie viele Menschen leben in den USA?\"\n",
        "# prompt = \"How many people live in the US?\"\n",
        "# prompt = \"What is the best selling song in the music store?\"\n",
        "# prompt = \"What is the cheapest album in the music store? Tell me the price.\"\n",
        "# prompt = \"What is the best selling song starting with the letter 'F' in the music store? Tell me title and artist.\"\n",
        "# prompt = \"How is the weather now in Cologne? Please use Celsius as unit for temperature.\"\n",
        "response = agent_executor.invoke({\"input\": prompt})\n",
        "\n",
        "# Response-Objekte:\n",
        "print()\n",
        "print(textwrap.fill(str(response[\"input\"]), 80))\n",
        "print()\n",
        "print(textwrap.fill(str(response[\"output\"]), 80))\n",
        "print()\n",
        "chat_history_string = str(response[\"chat_history\"]).replace(\"), \", \")§§§ \")[1:-1]\n",
        "chat_history = chat_history_string.split(\"§§§ \")\n",
        "msg_history = []\n",
        "for chat_string in chat_history:\n",
        "  tmp = chat_string.split(\"(content='\")\n",
        "  msg_history.append({\"role\": tmp[0], \"content\": tmp[1][:-2]})\n",
        "for msg in msg_history:\n",
        "  print(msg)"
      ],
      "metadata": {
        "id": "EA-3_QJj6tWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(textwrap.fill(str(response[\"input\"]), 80))\n",
        "print()\n",
        "print(textwrap.fill(str(response[\"output\"]), 80))\n",
        "print()\n",
        "chat_history_string = str(response[\"chat_history\"]).replace(\"), \", \")§§§ \")[1:-1]\n",
        "chat_history = chat_history_string.split(\"§§§ \")\n",
        "msg_history = []\n",
        "for chat_string in chat_history:\n",
        "  tmp = chat_string.split(\"(content='\")\n",
        "  msg_history.append({\"role\": tmp[0], \"content\": tmp[1][:-2]})\n",
        "for msg in msg_history:\n",
        "  print(msg)"
      ],
      "metadata": {
        "id": "9Zsnvgf6BRaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLI for agent_executor (Chatbot with memory)"
      ],
      "metadata": {
        "id": "-fL3b1FXIx9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory)\n",
        "messages = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n",
        "langchain.debug = False"
      ],
      "metadata": {
        "id": "qVfrjxhuUbn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt = \"Give me the premium for an insured sum of 10000000\"\n",
        "prompt = \"The industry is chemical production\"\n",
        "#prompt = \"And the temperature in Celsius?\"\n",
        "messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "print(\"user: \" + prompt)\n",
        "response = agent_executor.invoke({\"input\": prompt})\n",
        "msg = {\"role\": \"assistant\", \"content\": response[\"output\"]}\n",
        "messages.append(msg)\n",
        "print(textwrap.fill(\"assistant: \" + response[\"output\"], 80))\n",
        "print(\"\\nHistory:\")\n",
        "for msg in messages:\n",
        "    #st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
        "    print(textwrap.fill(msg[\"role\"] + \": \" + msg[\"content\"], 80))"
      ],
      "metadata": {
        "id": "OjDvg8GSaSP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Memory from response object:\")\n",
        "chat_history_string = str(response[\"chat_history\"]).replace(\"), \", \")§§§ \")[1:-1]\n",
        "chat_history = chat_history_string.split(\"§§§ \")\n",
        "msg_history = []\n",
        "for chat_string in chat_history:\n",
        "  tmp = chat_string.split(\"(content='\")\n",
        "  msg_history.append({\"role\": tmp[0], \"content\": tmp[1][:-2]})\n",
        "for msg in msg_history:\n",
        "  print(msg)"
      ],
      "metadata": {
        "id": "ZmCuWwuijWDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLI for agent (Chatbot with memory)"
      ],
      "metadata": {
        "id": "dnhhXmq0sYVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_kwargs = {\n",
        "    \"extra_prompt_messages\": [MessagesPlaceholder(variable_name=\"chat_history\")],\n",
        "}\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.OPENAI_FUNCTIONS,\n",
        "    verbose=False,\n",
        "    agent_kwargs=agent_kwargs,\n",
        "    memory=memory,\n",
        ")\n",
        "langchain.debug = False"
      ],
      "metadata": {
        "id": "Xs3oopFVsdXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"hi\")\n",
        "agent.run(\"My name is Bob\")\n",
        "agent.run(\"Wie lautet mein Name?\")\n",
        "agent.run(\"Give me the premium for an insured sum of 10000000.\")\n",
        "agent.run(\"it is metal construction industry\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lKxJyc-CskpC",
        "outputId": "18592e81-4e57-48c4-96b8-93bd51bd092d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The premium for an insured sum of 10,000,000 in the metal construction industry is 2,500,000.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Work in progress..."
      ],
      "metadata": {
        "id": "ZQ02m2ARb8cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING\"] = \"true\"\n",
        "llm = ChatOpenAI(temperature=0, model='gpt-4-0613')"
      ],
      "metadata": {
        "id": "Zs1dGiVGcmtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool 1\n",
        "def multiplier(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply the provided floats.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "tool = StructuredTool.from_function(multiplier)\n",
        "\n",
        "agent_executor = initialize_agent(\n",
        "    [tool],\n",
        "    llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")\n",
        "# The one Agent than can accept a structured tool with multiple arguments is the STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION Agent type\n",
        "# add memory: https://python.langchain.com/docs/modules/agents/agent_types/structured_chat#adding-in-memory\n",
        "\n",
        "prompt = \"What is 3 times 4\"\n",
        "response_text = agent_executor.run(prompt)\n",
        "print(response_text)\n",
        "response = agent_executor.invoke({\"input\": prompt})\n",
        "#print(textwrap.fill(str(response[\"output\"]), 80))\n"
      ],
      "metadata": {
        "id": "LtfgVAjZc14k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool 2\n",
        "def calculate_insurance_premium(insured_sum: int, industry: str) -> float:\n",
        "    \"\"\"Calculate the premium for an insurance based on the maximum insured sum and the industry of the customer.\"\"\"\n",
        "    premium = insured_sum*0.25\n",
        "    return premium\n",
        "\n",
        "tool = StructuredTool.from_function(calculate_insurance_premium, return_direct=False)\n",
        "\n",
        "agent_executor = initialize_agent(\n",
        "    [tool],\n",
        "    llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "prompt = \"Give me the premium for an insured sum of 10000000\"\n",
        "prompt = \"Kalkuliere die Versicherungsprämie für eine Versicherungssumme von 10000000, die Branche ist Metallbau\"\n",
        "response_text = agent_executor.run(prompt)\n",
        "print(response_text)\n",
        "response = agent_executor.invoke({\"input\": prompt})\n",
        "#print(textwrap.fill(str(response[\"output\"]), 80))\n"
      ],
      "metadata": {
        "id": "dF2g6iTCdM0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual loop for agent_executor"
      ],
      "metadata": {
        "id": "CS2Q0VjjxpIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 1\n",
        "\n",
        "Works only for single prompt - no memory!"
      ],
      "metadata": {
        "id": "bDrV8KoYN2s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://levelup.gitconnected.com/building-a-powerful-agent-has-no-challenge-today-774e27be818d\n",
        "\n",
        "# only for single prompt - no memory !\n",
        "\n",
        "def calculate_insurance_premium(insured_sum: int, industry: str) -> float:\n",
        "    \"\"\"Calculate the premium for an insurance based on the maximum insured sum and the industry of the customer.\"\"\"\n",
        "    premium = insured_sum*0.25\n",
        "    return premium\n",
        "\n",
        "calc_tool = StructuredTool.from_function(calculate_insurance_premium, return_direct=False)\n",
        "\n",
        "tools = [calc_tool]\n",
        "tools_dict = {\"calculate_insurance_premium\": calc_tool}\n",
        "\n",
        "llm_with_tools = llm.bind(\n",
        "    functions=[format_tool_to_openai_function(t) for t in tools]\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a useful assistant.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "agent = {\n",
        "    \"input\": lambda x: x[\"input\"],\n",
        "    \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps'])\n",
        "} | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()\n",
        "\n",
        "langchain.debug = False\n",
        "\n",
        "intermediate_steps = []\n",
        "while True:\n",
        "    output = agent.invoke({\n",
        "        \"input\": \"Kalkuliere die Versicherungsprämie für eine Versicherungssumme von 10000000, die Branche ist Metallbau.\",\n",
        "        \"intermediate_steps\": intermediate_steps\n",
        "    })\n",
        "    if isinstance(output, AgentFinish):\n",
        "        final_result = output.return_values[\"output\"]\n",
        "        break\n",
        "    else:\n",
        "        print(output.tool, output.tool_input)\n",
        "        #tool = {\n",
        "        #    \"get_word_length\": get_word_length\n",
        "        #}[output.tool]\n",
        "        tool = tools_dict[output.tool]\n",
        "        observation = tool.run(output.tool_input)\n",
        "        intermediate_steps.append((output, observation))\n",
        "\n",
        "print(final_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq3KyyEI-bYO",
        "outputId": "e5019897-4d65-4dd2-f8c0-0953a24e4ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculate_insurance_premium {'insured_sum': 10000000, 'industry': 'Metallbau'}\n",
            "Die Versicherungsprämie für eine Versicherungssumme von 10.000.000 in der Branche Metallbau beträgt 2.500.000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test 2\n",
        "\n",
        "Not working, memory implementation throws error"
      ],
      "metadata": {
        "id": "dAsubqpZN9i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseMessage\n",
        "hhh = BaseMessage(type=(\"User\"), content=(\"test\"))"
      ],
      "metadata": {
        "id": "45lfBmP5iAjo"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see https://levelup.gitconnected.com/building-a-powerful-agent-has-no-challenge-today-774e27be818d\n",
        "\n",
        "# only for single prompt - no memory !\n",
        "\n",
        "def calculate_insurance_premium(insured_sum: int, industry: str) -> float:\n",
        "    \"\"\"Calculate the premium for an insurance based on the maximum insured sum and the industry of the customer.\"\"\"\n",
        "    premium = insured_sum*0.25\n",
        "    return premium\n",
        "\n",
        "calc_tool = StructuredTool.from_function(calculate_insurance_premium, return_direct=False)\n",
        "\n",
        "tools = [calc_tool]\n",
        "tools_dict = {\"calculate_insurance_premium\": calc_tool}\n",
        "\n",
        "llm_with_tools = llm.bind(\n",
        "    functions=[format_tool_to_openai_function(t) for t in tools]\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a useful assistant.\"),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    (\"user\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])\n",
        "\n",
        "agent = {\n",
        "    \"input\": lambda x: x[\"input\"],\n",
        "    \"agent_scratchpad\": lambda x: format_to_openai_functions(x['intermediate_steps']),\n",
        "    \"chat_history\": lambda x: x[\"chat_history\"]\n",
        "} | prompt | llm_with_tools | OpenAIFunctionsAgentOutputParser()\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "langchain.debug = False\n",
        "\n",
        "intermediate_steps = []\n",
        "chat_history = []\n",
        "\n",
        "myprompt = \"Kalkuliere die Versicherungsprämie für eine Versicherungssumme von 10000000\"\n",
        "\n",
        "while True:\n",
        "    output = agent.invoke({\n",
        "        \"input\": myprompt,\n",
        "        \"intermediate_steps\": intermediate_steps,\n",
        "        \"chat_history\": memory\n",
        "    })\n",
        "    if isinstance(output, AgentFinish):\n",
        "        final_result = output.return_values[\"output\"]\n",
        "        # hier der Fehler? variable chat_history should be a list of base messages\n",
        "        #msg = {\"role\": \"user\", \"content\": myprompt}\n",
        "        chat_history.append(BaseMessage(type=(\"User\"), content=(myprompt)))\n",
        "        #msg = {\"role\": \"assistant\", \"content\": output.return_values[\"output\"]}\n",
        "        chat_history.append(BaseMessage(type=(\"assistant\"), content=(output.return_values[\"output\"])))\n",
        "        break\n",
        "    else:\n",
        "        print(output.tool, output.tool_input)\n",
        "        print(output)\n",
        "        #tool = {\n",
        "        #    \"get_word_length\": get_word_length\n",
        "        #}[output.tool]\n",
        "        tool = tools_dict[output.tool]\n",
        "        observation = tool.run(output.tool_input)\n",
        "        intermediate_steps.append((output, observation))\n",
        "\n",
        "print(final_result)"
      ],
      "metadata": {
        "id": "kBP6IHULEJ2P",
        "outputId": "f5ed8924-80b7-479e-8867-174f386ffaa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-13a4ad5f85cb>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     output = agent.invoke({\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmyprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34m\"intermediate_steps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mintermediate_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/runnable/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   1042\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/prompt_template.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPromptValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         return self._call_with_config(\n\u001b[0m\u001b[1;32m     58\u001b[0m             lambda inner_input: self.format_prompt(\n\u001b[1;32m     59\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/runnable/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m         )\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/runnable/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/schema/prompt_template.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPromptValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         return self._call_with_config(\n\u001b[0;32m---> 58\u001b[0;31m             lambda inner_input: self.format_prompt(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             ),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/prompts/chat.py\u001b[0m in \u001b[0;36mformat_prompt\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPromptValue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mChatPromptValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/prompts/chat.py\u001b[0m in \u001b[0;36mformat_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessage_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                 }\n\u001b[0;32m--> 583\u001b[0;31m                 \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/prompts/chat.py\u001b[0m in \u001b[0;36mformat_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"variable {self.variable_name} should be a list of base messages, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\"got {value}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: variable chat_history should be a list of base messages, got return_messages=True memory_key='chat_history'"
          ]
        }
      ]
    }
  ]
}